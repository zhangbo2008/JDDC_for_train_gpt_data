{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35TbrhCn8gGZ",
        "outputId": "76af0dd4-fee2-45be-965b-41090d2cb259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ChatGLM-6B'...\n",
            "remote: Enumerating objects: 1167, done.\u001b[K\n",
            "remote: Counting objects: 100% (603/603), done.\u001b[K\n",
            "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
            "remote: Total 1167 (delta 522), reused 537 (delta 499), pack-reused 564\u001b[K\n",
            "Receiving objects: 100% (1167/1167), 9.01 MiB | 7.95 MiB/s, done.\n",
            "Resolving deltas: 100% (691/691), done.\n",
            "Cloning into 'chatglm_test_data'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (3/3), 1.11 KiB | 1.11 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/THUDM/ChatGLM-6B\n",
        "!git clone https://github.com/zhangbo2008/chatglm_test_data\n",
        "!git clone https://github.com/zhangbo2008/JDDC_for_train_gpt_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd ChatGLM-6B/ptuning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRJBRGEi8rSz",
        "outputId": "fa03f3ff-91bf-4a30-e5fd-3a8ee990963a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ChatGLM-6B/ptuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#下面链接里面id=xxxxxxxxx    xxxx是谷歌盘里面的链接里面的id. 看链接就能看到贴上去即可.\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=13_vf0xRTQsyneRKdD1bZIr93vBGOczrk' -O xxx_file\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54BDjrvFCLAl",
        "outputId": "97f689cc-c7b9-495f-9976-3550729848d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-22 02:12:21--  https://docs.google.com/uc?export=download&id=13_vf0xRTQsyneRKdD1bZIr93vBGOczrk\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.200.101, 74.125.200.100, 74.125.200.139, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.200.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-0k-c8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8od1p506fjj1g302iu8bp9a4083pvo6i/1684721475000/12494598342230677309/*/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk?e=download&uuid=38e5e15e-b14a-4c19-96f8-7d116fe5fe1f [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-05-22 02:12:25--  https://doc-0k-c8-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/8od1p506fjj1g302iu8bp9a4083pvo6i/1684721475000/12494598342230677309/*/13_vf0xRTQsyneRKdD1bZIr93vBGOczrk?e=download&uuid=38e5e15e-b14a-4c19-96f8-7d116fe5fe1f\n",
            "Resolving doc-0k-c8-docs.googleusercontent.com (doc-0k-c8-docs.googleusercontent.com)... 142.251.12.132, 2404:6800:4003:c11::84\n",
            "Connecting to doc-0k-c8-docs.googleusercontent.com (doc-0k-c8-docs.googleusercontent.com)|142.251.12.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17069994 (16M) [application/x-gzip]\n",
            "Saving to: ‘xxx_file’\n",
            "\n",
            "xxx_file            100%[===================>]  16.28M  77.1MB/s    in 0.2s    \n",
            "\n",
            "2023-05-22 02:12:25 (77.1 MB/s) - ‘xxx_file’ saved [17069994/17069994]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tar zxvf  xxx_file  #解压上面的下载文件."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tqFOVbRCUca",
        "outputId": "e9e055e5-202e-4393-c3b9-bcafb46ab371"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdvertiseGen/\n",
            "AdvertiseGen/train.json\n",
            "AdvertiseGen/dev.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/ChatGLM-6B/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-7aEFFFE7Uw",
        "outputId": "61ba6f10-afc4-4049-e434-6fc05ba1f995"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r /content/ChatGLM-6B/requirements.txt (line 1)) (3.20.3)\n",
            "Collecting transformers==4.27.1 (from -r /content/ChatGLM-6B/requirements.txt (line 2))\n",
            "  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cpm_kernels (from -r /content/ChatGLM-6B/requirements.txt (line 3))\n",
            "  Downloading cpm_kernels-1.0.11-py3-none-any.whl (416 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m416.6/416.6 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from -r /content/ChatGLM-6B/requirements.txt (line 4)) (2.0.1+cu118)\n",
            "Collecting gradio (from -r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading gradio-3.32.0-py3-none-any.whl (19.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.9/19.9 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mdtex2html (from -r /content/ChatGLM-6B/requirements.txt (line 6))\n",
            "  Downloading mdtex2html-1.2.0-py3-none-any.whl (13 kB)\n",
            "Collecting sentencepiece (from -r /content/ChatGLM-6B/requirements.txt (line 7))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from -r /content/ChatGLM-6B/requirements.txt (line 8))\n",
            "  Downloading accelerate-0.19.0-py3-none-any.whl (219 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.1/219.1 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2))\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m126.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (4.65.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (16.0.5)\n",
            "Collecting aiofiles (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (3.8.4)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (4.2.2)\n",
            "Collecting fastapi (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading fastapi-0.95.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.0/57.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client>=0.2.4 (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading gradio_client-0.2.5-py3-none-any.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.1/288.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown-it-py[linkify]>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2.2.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (3.7.1)\n",
            "Collecting mdit-py-plugins<=0.3.3 (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading orjson-3.8.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.5.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (8.4.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.10.7)\n",
            "Collecting pydub (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: pygments>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2.14.0)\n",
            "Collecting python-multipart (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading uvicorn-0.22.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.0 (from gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from mdtex2html->-r /content/ChatGLM-6B/requirements.txt (line 6)) (3.4.3)\n",
            "Collecting latex2mathml (from mdtex2html->-r /content/ChatGLM-6B/requirements.txt (line 6))\n",
            "  Downloading latex2mathml-3.76.0-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r /content/ChatGLM-6B/requirements.txt (line 8)) (5.9.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (4.3.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair>=4.2.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client>=0.2.4->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2023.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py[linkify]>=2.0.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.1.2)\n",
            "Collecting linkify-it-py<3,>=1 (from markdown-it-py[linkify]>=2.0.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading linkify_it_py-2.0.2-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2022.7.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (8.1.3)\n",
            "Collecting h11>=0.8 (from uvicorn>=0.14.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.3.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (2022.12.7)\n",
            "Collecting httpcore<0.18.0,>=0.15.0 (from httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading httpcore-0.17.1-py3-none-any.whl (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.9/70.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (3.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.27.1->-r /content/ChatGLM-6B/requirements.txt (line 2)) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->-r /content/ChatGLM-6B/requirements.txt (line 4)) (1.3.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (3.6.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (0.19.3)\n",
            "Collecting uc-micro-py (from linkify-it-py<3,>=1->markdown-it-py[linkify]>=2.0.0->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5))\n",
            "  Downloading uc_micro_py-1.0.2-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->gradio->-r /content/ChatGLM-6B/requirements.txt (line 5)) (1.16.0)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4694 sha256=6d775a6348fc58433d1a9725a869ca9330117cf044420c3d1d2f551cffc8af9f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/c2/0e/3b9c6845c6a4e35beb90910cc70d9ac9ab5d47402bd62af0df\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: tokenizers, sentencepiece, pydub, ffmpy, cpm_kernels, websockets, uc-micro-py, semantic-version, python-multipart, orjson, latex2mathml, h11, aiofiles, uvicorn, starlette, mdtex2html, mdit-py-plugins, linkify-it-py, httpcore, transformers, httpx, fastapi, gradio-client, gradio, accelerate\n",
            "Successfully installed accelerate-0.19.0 aiofiles-23.1.0 cpm_kernels-1.0.11 fastapi-0.95.2 ffmpy-0.3.0 gradio-3.32.0 gradio-client-0.2.5 h11-0.14.0 httpcore-0.17.1 httpx-0.24.1 latex2mathml-3.76.0 linkify-it-py-2.0.2 mdit-py-plugins-0.3.3 mdtex2html-1.2.0 orjson-3.8.12 pydub-0.25.1 python-multipart-0.0.6 semantic-version-2.10.0 sentencepiece-0.1.99 starlette-0.27.0 tokenizers-0.13.3 transformers-4.27.1 uc-micro-py-1.0.2 uvicorn-0.22.0 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#==========把这个文件写入main.py里面方便每次修改.\n",
        "\"\"\"\n",
        "Fine-tuning the library models for sequence to sequence.\n",
        "\"\"\"\n",
        "\n",
        "aaa='''\n",
        "print('使用了动态修改的版本!!!!!!!!!!!')\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import jieba \n",
        "from rouge_chinese import Rouge\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    HfArgumentParser,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "from trainer_seq2seq import Seq2SeqTrainer\n",
        "\n",
        "from arguments import ModelArguments, DataTrainingArguments\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n",
        "    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n",
        "        # If we pass only one argument to the script and it's the path to a json file,\n",
        "        # let's parse it to get our arguments.\n",
        "        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n",
        "    else:\n",
        "        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
        "\n",
        "    # Setup logging\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        handlers=[logging.StreamHandler(sys.stdout)],\n",
        "    )\n",
        "\n",
        "    if training_args.should_log:\n",
        "        # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "\n",
        "    log_level = training_args.get_process_log_level()\n",
        "    logger.setLevel(log_level)\n",
        "    # datasets.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.set_verbosity(log_level)\n",
        "    transformers.utils.logging.enable_default_handler()\n",
        "    transformers.utils.logging.enable_explicit_format()\n",
        "\n",
        "    # Log on each process the small summary:\n",
        "    logger.warning(\n",
        "        f\"Process rank: {training_args.local_rank}, device: {training_args.device}, n_gpu: {training_args.n_gpu}\"\n",
        "        + f\"distributed training: {bool(training_args.local_rank != -1)}, 16-bits training: {training_args.fp16}\"\n",
        "    )\n",
        "    logger.info(f\"Training/evaluation parameters {training_args}\")\n",
        "\n",
        "    # Set seed before initializing model.\n",
        "    set_seed(training_args.seed)\n",
        "\n",
        "    # Load dataset\n",
        "    data_files = {}\n",
        "    if data_args.train_file is not None:\n",
        "        data_files[\"train\"] = data_args.train_file\n",
        "        extension = data_args.train_file.split(\".\")[-1]\n",
        "    if data_args.validation_file is not None:\n",
        "        data_files[\"validation\"] = data_args.validation_file\n",
        "        extension = data_args.validation_file.split(\".\")[-1]\n",
        "    if data_args.test_file is not None:\n",
        "        data_files[\"test\"] = data_args.test_file\n",
        "        extension = data_args.test_file.split(\".\")[-1]\n",
        "\n",
        "    raw_datasets = load_dataset(\n",
        "        extension,\n",
        "        data_files=data_files,\n",
        "        cache_dir=model_args.cache_dir,\n",
        "        use_auth_token=True if model_args.use_auth_token else None,\n",
        "    )\n",
        "\n",
        "    # Load pretrained model and tokenizer\n",
        "    config = AutoConfig.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
        "    config.pre_seq_len = model_args.pre_seq_len\n",
        "    config.prefix_projection = model_args.prefix_projection\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, trust_remote_code=True)\n",
        "\n",
        "    if model_args.ptuning_checkpoint is not None:\n",
        "        # Evaluation\n",
        "        # Loading extra state dict of prefix encoder\n",
        "        model = AutoModel.from_pretrained(model_args.model_name_or_path, config=config, trust_remote_code=True)\n",
        "        prefix_state_dict = torch.load(os.path.join(model_args.ptuning_checkpoint, \"pytorch_model.bin\"))\n",
        "        new_prefix_state_dict = {}\n",
        "        for k, v in prefix_state_dict.items():\n",
        "            if k.startswith(\"transformer.prefix_encoder.\"):\n",
        "                new_prefix_state_dict[k[len(\"transformer.prefix_encoder.\"):]] = v\n",
        "        model.transformer.prefix_encoder.load_state_dict(new_prefix_state_dict)\n",
        "    else:\n",
        "        model = AutoModel.from_pretrained(model_args.model_name_or_path, config=config, trust_remote_code=True)\n",
        "\n",
        "    if model_args.quantization_bit is not None:\n",
        "        print(f\"Quantized to {model_args.quantization_bit} bit\")\n",
        "        model = model.quantize(model_args.quantization_bit)\n",
        "    if model_args.pre_seq_len is not None:\n",
        "        # P-tuning v2\n",
        "        model = model.half()\n",
        "        model.transformer.prefix_encoder.float()\n",
        "    else:\n",
        "        # Finetune\n",
        "        model = model.float()\n",
        "\n",
        "    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n",
        "\n",
        "    # Preprocessing the datasets.\n",
        "    # We need to tokenize inputs and targets.\n",
        "    if training_args.do_train:\n",
        "        column_names = raw_datasets[\"train\"].column_names\n",
        "    elif training_args.do_eval:\n",
        "        column_names = raw_datasets[\"validation\"].column_names\n",
        "    elif training_args.do_predict:\n",
        "        column_names = raw_datasets[\"test\"].column_names\n",
        "    else:\n",
        "        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n",
        "        return\n",
        "\n",
        "    # Get the column names for input/target.\n",
        "    prompt_column = data_args.prompt_column\n",
        "    response_column = data_args.response_column\n",
        "    history_column = data_args.history_column\n",
        "    \n",
        "    # Temporarily set max_target_length for training.\n",
        "    max_target_length = data_args.max_target_length\n",
        "\n",
        "    def preprocess_function_eval(examples):\n",
        "        inputs, targets = [], []\n",
        "        for i in range(len(examples[prompt_column])):\n",
        "            if examples[prompt_column][i] and examples[response_column][i]:\n",
        "                query = examples[prompt_column][i]\n",
        "                if history_column is None or len(examples[history_column][i]) == 0:\n",
        "                    prompt = query\n",
        "                else:\n",
        "                    prompt = \"\"\n",
        "                    history = examples[history_column][i]\n",
        "                    for turn_idx, (old_query, response) in enumerate(history):\n",
        "                        prompt += \"[Round {}]\\\n",
        "问：{}\\\n",
        "答：{}\\\n",
        "\".format(turn_idx, old_query, response)\n",
        "                    prompt += \"[Round {}]\\\n",
        "问：{}\\\n",
        "答：\".format(len(history), query)\n",
        "                inputs.append(prompt)\n",
        "                targets.append(examples[response_column][i])\n",
        "\n",
        "        inputs = [prefix + inp for inp in inputs]\n",
        "        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, truncation=True, padding=True)\n",
        "        labels = tokenizer(text_target=targets, max_length=max_target_length, truncation=True)\n",
        "\n",
        "        if data_args.ignore_pad_token_for_loss:\n",
        "            labels[\"input_ids\"] = [\n",
        "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n",
        "            ]\n",
        "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    def preprocess_function_train(examples):\n",
        "        max_seq_length = data_args.max_source_length + data_args.max_target_length\n",
        "\n",
        "        model_inputs = {\n",
        "            \"input_ids\": [],\n",
        "            \"labels\": [],\n",
        "        }\n",
        "        for i in range(len(examples[prompt_column])):\n",
        "            if examples[prompt_column][i] and examples[response_column][i]:\n",
        "                query, answer = examples[prompt_column][i], examples[response_column][i]\n",
        "\n",
        "                if history_column is None:\n",
        "                    prompt = query\n",
        "                else:\n",
        "                    prompt = \"\"\n",
        "                    history = examples[history_column][i]\n",
        "                    for turn_idx, (old_query, response) in enumerate(history):\n",
        "                        prompt += \"[Round {}]\\\\n问：{}\\\\n答：{}\\\\n\".format(turn_idx, old_query, response)\n",
        "                    prompt += \"[Round {}]\\\\n问：{}\\\\n答：\".format(len(history), query)\n",
        "\n",
        "                prompt = prefix + prompt\n",
        "                a_ids = tokenizer.encode(text=prompt, add_special_tokens=False)\n",
        "                b_ids = tokenizer.encode(text=answer, add_special_tokens=False)\n",
        "\n",
        "                if len(a_ids) > data_args.max_source_length - 1:\n",
        "                    a_ids = a_ids[: data_args.max_source_length - 1]\n",
        "\n",
        "                if len(b_ids) > data_args.max_target_length - 2:\n",
        "                    b_ids = b_ids[: data_args.max_target_length - 2]\n",
        "\n",
        "                input_ids = tokenizer.build_inputs_with_special_tokens(a_ids, b_ids)\n",
        "\n",
        "                context_length = input_ids.index(tokenizer.bos_token_id)\n",
        "                mask_position = context_length - 1\n",
        "                labels = [-100] * context_length + input_ids[mask_position+1:]\n",
        "                \n",
        "                pad_len = max_seq_length - len(input_ids)\n",
        "                input_ids = input_ids + [tokenizer.pad_token_id] * pad_len\n",
        "                labels = labels + [tokenizer.pad_token_id] * pad_len\n",
        "                if data_args.ignore_pad_token_for_loss:\n",
        "                    labels = [(l if l != tokenizer.pad_token_id else -100) for l in labels]\n",
        "\n",
        "                model_inputs[\"input_ids\"].append(input_ids)\n",
        "                model_inputs[\"labels\"].append(labels)\n",
        "\n",
        "        return model_inputs\n",
        "    \n",
        "    def print_dataset_example(example):\n",
        "        print(\"input_ids\",example[\"input_ids\"])\n",
        "        print(\"inputs\", tokenizer.decode(example[\"input_ids\"]))\n",
        "        print(\"label_ids\", example[\"labels\"])\n",
        "        print(\"labels\", tokenizer.decode(example[\"labels\"]))\n",
        "\n",
        "    if training_args.do_train:\n",
        "        if \"train\" not in raw_datasets:\n",
        "            raise ValueError(\"--do_train requires a train dataset\")\n",
        "        train_dataset = raw_datasets[\"train\"]\n",
        "        if data_args.max_train_samples is not None:\n",
        "            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
        "            train_dataset = train_dataset.select(range(max_train_samples))\n",
        "        with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
        "            train_dataset = train_dataset.map(\n",
        "                preprocess_function_train,\n",
        "                batched=True,\n",
        "                num_proc=data_args.preprocessing_num_workers,\n",
        "                remove_columns=column_names,\n",
        "                load_from_cache_file=not data_args.overwrite_cache,\n",
        "                desc=\"Running tokenizer on train dataset\",\n",
        "            )\n",
        "        print_dataset_example(train_dataset[0])\n",
        "\n",
        "    if training_args.do_eval:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"validation\" not in raw_datasets:\n",
        "            raise ValueError(\"--do_eval requires a validation dataset\")\n",
        "        eval_dataset = raw_datasets[\"validation\"]\n",
        "        if data_args.max_eval_samples is not None:\n",
        "            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
        "            eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "        with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
        "            eval_dataset = eval_dataset.map(\n",
        "                preprocess_function_eval,\n",
        "                batched=True,\n",
        "                num_proc=data_args.preprocessing_num_workers,\n",
        "                remove_columns=column_names,\n",
        "                load_from_cache_file=not data_args.overwrite_cache,\n",
        "                desc=\"Running tokenizer on validation dataset\",\n",
        "            )\n",
        "        print_dataset_example(eval_dataset[0])\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        max_target_length = data_args.val_max_target_length\n",
        "        if \"test\" not in raw_datasets:\n",
        "            raise ValueError(\"--do_predict requires a test dataset\")\n",
        "        predict_dataset = raw_datasets[\"test\"]\n",
        "        if data_args.max_predict_samples is not None:\n",
        "            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n",
        "            predict_dataset = predict_dataset.select(range(max_predict_samples))\n",
        "        with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n",
        "            predict_dataset = predict_dataset.map(\n",
        "                preprocess_function_eval,\n",
        "                batched=True,\n",
        "                num_proc=data_args.preprocessing_num_workers,\n",
        "                remove_columns=column_names,\n",
        "                load_from_cache_file=not data_args.overwrite_cache,\n",
        "                desc=\"Running tokenizer on prediction dataset\",\n",
        "            )\n",
        "        print_dataset_example(predict_dataset[0])\n",
        "\n",
        "    # Data collator\n",
        "    label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n",
        "    data_collator = DataCollatorForSeq2Seq(\n",
        "        tokenizer,\n",
        "        model=model,\n",
        "        label_pad_token_id=label_pad_token_id,\n",
        "        pad_to_multiple_of=None,\n",
        "        padding=False\n",
        "    )\n",
        "\n",
        "    # Metric\n",
        "    def compute_metrics(eval_preds):\n",
        "        preds, labels = eval_preds\n",
        "        if isinstance(preds, tuple):\n",
        "            preds = preds[0]\n",
        "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "        if data_args.ignore_pad_token_for_loss:\n",
        "            # Replace -100 in the labels as we can't decode them.\n",
        "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        score_dict = {\n",
        "            \"rouge-1\": [],\n",
        "            \"rouge-2\": [],\n",
        "            \"rouge-l\": [],\n",
        "            \"bleu-4\": []\n",
        "        }\n",
        "        for pred, label in zip(decoded_preds, decoded_labels):\n",
        "            hypothesis = list(jieba.cut(pred))\n",
        "            reference = list(jieba.cut(label))\n",
        "            rouge = Rouge()\n",
        "            scores = rouge.get_scores(' '.join(hypothesis) , ' '.join(reference))\n",
        "            result = scores[0]\n",
        "            \n",
        "            for k, v in result.items():\n",
        "                score_dict[k].append(round(v[\"f\"] * 100, 4))\n",
        "            bleu_score = sentence_bleu([list(label)], list(pred), smoothing_function=SmoothingFunction().method3)\n",
        "            score_dict[\"bleu-4\"].append(round(bleu_score * 100, 4))\n",
        "\n",
        "        for k, v in score_dict.items():\n",
        "            score_dict[k] = float(np.mean(v))\n",
        "        return score_dict\n",
        "\n",
        "    # Override the decoding parameters of Seq2SeqTrainer\n",
        "    training_args.generation_max_length = (\n",
        "        training_args.generation_max_length\n",
        "        if training_args.generation_max_length is not None\n",
        "        else data_args.val_max_target_length\n",
        "    )\n",
        "    training_args.generation_num_beams = (\n",
        "        data_args.num_beams if data_args.num_beams is not None else training_args.generation_num_beams\n",
        "    )\n",
        "    # Initialize our Trainer\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset if training_args.do_train else None,\n",
        "        eval_dataset=eval_dataset if training_args.do_eval else None,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=data_collator,\n",
        "        compute_metrics=compute_metrics if training_args.predict_with_generate else None,\n",
        "        save_prefixencoder=model_args.pre_seq_len is not None\n",
        "    )\n",
        "    #=========\n",
        "    results = {}\n",
        "    max_seq_length = data_args.max_source_length + data_args.max_target_length + 1\n",
        "    print('max_seq_length',max_seq_length,9999999999999999999999999999999999999999999)\n",
        "    print('训练之前的测试!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "    model = model.eval()\n",
        "    response, history = model.chat(tokenizer, \"长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线\", history=[])\n",
        "    print(response, history)\n",
        "    model=model.train()\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "        predict_results = trainer.predict(predict_dataset, metric_key_prefix=\"predict\", max_length=max_seq_length, do_sample=True, top_p=0.7, temperature=0.95)\n",
        "        metrics = predict_results.metrics\n",
        "        max_predict_samples = (\n",
        "            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n",
        "        )\n",
        "        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
        "\n",
        "        trainer.log_metrics(\"predict\", metrics)\n",
        "        trainer.save_metrics(\"predict\", metrics)\n",
        "\n",
        "        if trainer.is_world_process_zero():\n",
        "            if training_args.predict_with_generate:\n",
        "                predictions = tokenizer.batch_decode(\n",
        "                    predict_results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                predictions = [pred.strip() for pred in predictions]\n",
        "                labels = tokenizer.batch_decode(\n",
        "                    predict_results.label_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                labels = [label.strip() for label in labels]\n",
        "                output_prediction_file = os.path.join(training_args.output_dir, \"generated_predictions.txt\")\n",
        "                with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "                    for p, l in zip(predictions, labels):\n",
        "                        res = json.dumps({\"labels\": l, \"predict\": p}, ensure_ascii=False)\n",
        "                        writer.write(f\"{res}\\\\n\")\n",
        "    # Training\n",
        "    if training_args.do_train:\n",
        "        checkpoint = None\n",
        "        if training_args.resume_from_checkpoint is not None:\n",
        "            checkpoint = training_args.resume_from_checkpoint\n",
        "        # elif last_checkpoint is not None:\n",
        "        #     checkpoint = last_checkpoint\n",
        "        model.gradient_checkpointing_enable()\n",
        "        model.enable_input_require_grads()\n",
        "        train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
        "        # trainer.save_model()  # Saves the tokenizer too for easy upload\n",
        "\n",
        "        metrics = train_result.metrics\n",
        "        max_train_samples = (\n",
        "            data_args.max_train_samples if data_args.max_train_samples is not None else len(train_dataset)\n",
        "        )\n",
        "        metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
        "\n",
        "        trainer.log_metrics(\"train\", metrics)\n",
        "        trainer.save_metrics(\"train\", metrics)\n",
        "        trainer.save_state()\n",
        "\n",
        "    # Evaluation\n",
        "    results = {}\n",
        "    max_seq_length = data_args.max_source_length + data_args.max_target_length + 1\n",
        "    if training_args.do_eval:\n",
        "        logger.info(\"*** Evaluate ***\")\n",
        "        metrics = trainer.evaluate(metric_key_prefix=\"eval\", do_sample=True, top_p=0.7, max_length=max_seq_length, temperature=0.95)\n",
        "        max_eval_samples = data_args.max_eval_samples if data_args.max_eval_samples is not None else len(eval_dataset)\n",
        "        metrics[\"eval_samples\"] = min(max_eval_samples, len(eval_dataset))\n",
        "\n",
        "        trainer.log_metrics(\"eval\", metrics)\n",
        "        trainer.save_metrics(\"eval\", metrics)\n",
        "\n",
        "    if training_args.do_predict:\n",
        "        logger.info(\"*** Predict ***\")\n",
        "        predict_results = trainer.predict(predict_dataset, metric_key_prefix=\"predict\", max_length=max_seq_length, do_sample=True, top_p=0.7, temperature=0.95)\n",
        "        metrics = predict_results.metrics\n",
        "        max_predict_samples = (\n",
        "            data_args.max_predict_samples if data_args.max_predict_samples is not None else len(predict_dataset)\n",
        "        )\n",
        "        metrics[\"predict_samples\"] = min(max_predict_samples, len(predict_dataset))\n",
        "\n",
        "        trainer.log_metrics(\"predict\", metrics)\n",
        "        trainer.save_metrics(\"predict\", metrics)\n",
        "\n",
        "        if trainer.is_world_process_zero():\n",
        "            if training_args.predict_with_generate:\n",
        "                predictions = tokenizer.batch_decode(\n",
        "                    predict_results.predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                predictions = [pred.strip() for pred in predictions]\n",
        "                labels = tokenizer.batch_decode(\n",
        "                    predict_results.label_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "                )\n",
        "                labels = [label.strip() for label in labels]\n",
        "                output_prediction_file = os.path.join(training_args.output_dir, \"generated_predictions.txt\")\n",
        "                with open(output_prediction_file, \"w\", encoding=\"utf-8\") as writer:\n",
        "                    for p, l in zip(predictions, labels):\n",
        "                        res = json.dumps({\"labels\": l, \"predict\": p}, ensure_ascii=False)\n",
        "                        writer.write(f\"{res}\\\\n\")\n",
        "    print('训练之后的测试!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
        "    model = model.eval()\n",
        "\n",
        "\n",
        "\n",
        "    print('打印测试集')\n",
        "    t=raw_datasets[\"test\"]\n",
        "    for i in t:\n",
        "      print(i)\n",
        "    print(t,333333333333333333333333333333333333)\n",
        "    print('下面打印评测结果在测试集上全部进行测试')\n",
        "\n",
        "\n",
        "    for dex,i in enumerate(t):\n",
        "      print('第',dex,'条数据进行测是')\n",
        "      print('问题是',i['prompt'],'历史是',i['history'])\n",
        "      print('答案是',i['response'])\n",
        "      response, history = model.chat(tokenizer, i['prompt'], history=i['history'])\n",
        "      print('回答是',response, )\n",
        "    print('测试完毕')\n",
        "    model=model.train()\n",
        "    return results\n",
        "\n",
        "\n",
        "def _mp_fn(index):\n",
        "    # For xla_spawn (TPUs)\n",
        "    main()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "import os\n",
        "with open('main.py','w') as f:\n",
        "    f.write(aaa)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!pip install datasets\n",
        "!pip install rouge_chinese\n",
        "\n",
        "%env PRE_SEQ_LEN=128          \n",
        "%env LR=2e-2\n",
        "!echo $LR\n",
        "!echo 33333333333333333333333333333333333333333333\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 main.py \\\n",
        "    --do_train \\\n",
        "    --do_predict False \\\n",
        "    --train_file /content/chatglm_test_data/train.json \\\n",
        "    --test_file /content/chatglm_test_data/train.json \\\n",
        "    --prompt_column prompt \\\n",
        "    --response_column response \\\n",
        "    --history_column history\\\n",
        "    --overwrite_cache \\\n",
        "    --model_name_or_path THUDM/chatglm-6b-int4 \\\n",
        "    --output_dir output/adgen-chatglm-6b-pt-$PRE_SEQ_LEN-$LR \\\n",
        "    --overwrite_output_dir \\\n",
        "    --max_source_length 64 \\\n",
        "    --max_target_length 64 \\\n",
        "    --per_device_train_batch_size 1 \\\n",
        "    --per_device_eval_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 16 \\\n",
        "    --predict_with_generate \\\n",
        "    --max_steps 0 \\\n",
        "    --num_train_epochs 30\\\n",
        "    --logging_steps 10 \\\n",
        "    --save_steps 1000 \\\n",
        "    --learning_rate $LR \\\n",
        "    --pre_seq_len $PRE_SEQ_LEN \\\n",
        "    --quantization_bit 4\\\n",
        "\n",
        "#==========参数解释: max_steps 设置为0, 不让他起作用.num_train_epochs 表示一共跑3轮次.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFcdcjFyDuLc",
        "outputId": "4e70f3a7-3164-4609-ce85-968b6d4bf098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.4.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: rouge_chinese in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge_chinese) (1.16.0)\n",
            "env: PRE_SEQ_LEN=128\n",
            "env: LR=2e-2\n",
            "2e-2\n",
            "33333333333333333333333333333333333333333333\n",
            "使用了动态修改的版本!!!!!!!!!!!\n",
            "2023-05-22 03:07:41.433941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "05/22/2023 03:07:42 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "05/22/2023 03:07:42 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=16,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=0.02,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=output/adgen-chatglm-6b-pt-128-2e-2/runs/May22_03-07-42_f381f9b5beb6,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=10,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=0,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=30.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=output/adgen-chatglm-6b-pt-128-2e-2,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=1,\n",
            "per_device_train_batch_size=1,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=output/adgen-chatglm-6b-pt-128-2e-2,\n",
            "save_on_each_node=False,\n",
            "save_steps=1000,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "05/22/2023 03:07:43 - WARNING - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-3dfb46796f7748b4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n",
            "100% 2/2 [00:00<00:00, 723.47it/s]\n",
            "[INFO|configuration_utils.py:668] 2023-05-22 03:07:43,725 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b-int4/snapshots/02a065cf2797029c036a02cac30f1da1a9bc49a3/config.json\n",
            "[WARNING|configuration_auto.py:905] 2023-05-22 03:07:43,725 >> Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "[INFO|configuration_utils.py:668] 2023-05-22 03:07:44,448 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b-int4/snapshots/02a065cf2797029c036a02cac30f1da1a9bc49a3/config.json\n",
            "[INFO|configuration_utils.py:720] 2023-05-22 03:07:44,448 >> Model config ChatGLMConfig {\n",
            "  \"_name_or_path\": \"THUDM/chatglm-6b-int4\",\n",
            "  \"architectures\": [\n",
            "    \"ChatGLMModel\"\n",
            "  ],\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"configuration_chatglm.ChatGLMConfig\",\n",
            "    \"AutoModel\": \"modeling_chatglm.ChatGLMForConditionalGeneration\",\n",
            "    \"AutoModelForSeq2SeqLM\": \"modeling_chatglm.ChatGLMForConditionalGeneration\"\n",
            "  },\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"gmask_token_id\": 130001,\n",
            "  \"hidden_size\": 4096,\n",
            "  \"inner_hidden_size\": 16384,\n",
            "  \"layernorm_epsilon\": 1e-05,\n",
            "  \"mask_token_id\": 130000,\n",
            "  \"max_sequence_length\": 2048,\n",
            "  \"model_type\": \"chatglm\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_layers\": 28,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"position_encoding_2d\": true,\n",
            "  \"pre_seq_len\": null,\n",
            "  \"prefix_projection\": false,\n",
            "  \"quantization_bit\": 4,\n",
            "  \"quantization_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.27.1\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 130528\n",
            "}\n",
            "\n",
            "[WARNING|tokenization_auto.py:652] 2023-05-22 03:07:44,670 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-05-22 03:07:45,178 >> loading file ice_text.model from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b-int4/snapshots/02a065cf2797029c036a02cac30f1da1a9bc49a3/ice_text.model\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-05-22 03:07:45,178 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-05-22 03:07:45,178 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1802] 2023-05-22 03:07:45,178 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b-int4/snapshots/02a065cf2797029c036a02cac30f1da1a9bc49a3/tokenizer_config.json\n",
            "[WARNING|auto_factory.py:456] 2023-05-22 03:07:45,432 >> Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
            "[INFO|modeling_utils.py:2403] 2023-05-22 03:07:47,099 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--THUDM--chatglm-6b-int4/snapshots/02a065cf2797029c036a02cac30f1da1a9bc49a3/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:575] 2023-05-22 03:08:06,364 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.27.1\"\n",
            "}\n",
            "\n",
            "No compiled kernel found.\n",
            "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c\n",
            "Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "Using quantization cache\n",
            "Applying quantization to glm layers\n",
            "[INFO|modeling_utils.py:3032] 2023-05-22 03:08:09,763 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.\n",
            "\n",
            "[WARNING|modeling_utils.py:3034] 2023-05-22 03:08:09,763 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at THUDM/chatglm-6b-int4 and are newly initialized: ['transformer.prefix_encoder.embedding.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[INFO|modeling_utils.py:2690] 2023-05-22 03:08:10,100 >> Generation config file not found, using a generation config created from the model config.\n",
            "Quantized to 4 bit\n",
            "No compiled kernel found.\n",
            "Compiling kernels : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c\n",
            "Compiling gcc -O3 -fPIC -std=c99 /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.c -shared -o /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "Load kernel : /root/.cache/huggingface/modules/transformers_modules/THUDM/chatglm-6b-int4/02a065cf2797029c036a02cac30f1da1a9bc49a3/quantization_kernels.so\n",
            "input_ids [53, 6945, 5, 8, 42, 4, 64286, 12, 70266, 217, 13, 80690, 63848, 64191, 63823, 105269, 64831, 63823, 65368, 65020, 64831, 70826, 64992, 80690, 63843, 64992, 119909, 63823, 63872, 105269, 66503, 63866, 66108, 64029, 4, 67342, 12, 130001, 130004, 5, 113510, 63894, 64406, 64303, 64363, 63964, 31, 76116, 65047, 130005, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
            "inputs [Round 0]\n",
            "问:长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线\n",
            "答: 用电脑能读数据流吗?水温多少\n",
            "label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 130004, 5, 113510, 63894, 64406, 64303, 64363, 63964, 31, 76116, 65047, 130005, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
            "labels <image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100> 用电脑能读数据流吗?水温多少<image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100><image_-100>\n",
            "max_seq_length 129 9999999999999999999999999999999999999999999\n",
            "训练之前的测试!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "[INFO|configuration_utils.py:575] 2023-05-22 03:08:20,586 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.27.1\"\n",
            "}\n",
            "\n",
            "05/22/2023 03:08:20 - WARNING - transformers_modules.THUDM.chatglm-6b-int4.02a065cf2797029c036a02cac30f1da1a9bc49a3.modeling_chatglm - The dtype of attention mask (torch.int64) is not bool\n",
            "继电器和传感器的工作原理不同，它们的作用是控制电路中的电流流动。\n",
            "\n",
            "继电器是一种电子元件，可以控制电路中的电流流动，确保计算机内部硬件电路的安全。继电器通常由两个主要部分组成：一个控制电路和一组电子元件，例如电阻、电容和电感。这些元件组合在一起，可以形成一个电路，通过电流流动。\n",
            "\n",
            "如果继电器没有工作，电流无法流动。这意味着计算机内部硬件无法工作，因为它们需要电流来运行。\n",
            "\n",
            "另一方面，传感器是一种电子元件，可以检测并控制电流的流动。它们可以检测计算机内部硬件的状态，例如 CPU 温度、风扇转速等，然后向计算机发送一个信号，告诉计算机如何运行。\n",
            "\n",
            "因此，如果继电器没有控制电路，它无法确保计算机内部硬件的安全。同样，如果传感器没有检测到计算机内部硬件的状态，它也无法向计算机发送信号，这可能会导致计算机无法正常工作。\n",
            "\n",
            "因此，继电器和传感器在计算机中扮演着不同的角色，继电器负责控制电流流动，而传感器则负责检测计算机内部硬件的状态。如果继电器缺一个信号线，它可能无法检测到计算机内部硬件的状态，这可能导致计算机无法正常工作。 [('长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线', '继电器和传感器的工作原理不同，它们的作用是控制电路中的电流流动。\\n\\n继电器是一种电子元件，可以控制电路中的电流流动，确保计算机内部硬件电路的安全。继电器通常由两个主要部分组成：一个控制电路和一组电子元件，例如电阻、电容和电感。这些元件组合在一起，可以形成一个电路，通过电流流动。\\n\\n如果继电器没有工作，电流无法流动。这意味着计算机内部硬件无法工作，因为它们需要电流来运行。\\n\\n另一方面，传感器是一种电子元件，可以检测并控制电流的流动。它们可以检测计算机内部硬件的状态，例如 CPU 温度、风扇转速等，然后向计算机发送一个信号，告诉计算机如何运行。\\n\\n因此，如果继电器没有控制电路，它无法确保计算机内部硬件的安全。同样，如果传感器没有检测到计算机内部硬件的状态，它也无法向计算机发送信号，这可能会导致计算机无法正常工作。\\n\\n因此，继电器和传感器在计算机中扮演着不同的角色，继电器负责控制电流流动，而传感器则负责检测计算机内部硬件的状态。如果继电器缺一个信号线，它可能无法检测到计算机内部硬件的状态，这可能导致计算机无法正常工作。')]\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "  0% 0/30 [00:00<?, ?it/s]05/22/2023 03:09:12 - WARNING - transformers_modules.THUDM.chatglm-6b-int4.02a065cf2797029c036a02cac30f1da1a9bc49a3.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "{'loss': 0.5099, 'learning_rate': 0.013333333333333332, 'epoch': 9.0}\n",
            "{'loss': 0.0444, 'learning_rate': 0.006666666666666666, 'epoch': 18.0}\n",
            "{'loss': 0.0088, 'learning_rate': 0.0, 'epoch': 26.67}\n",
            "{'train_runtime': 57.8427, 'train_samples_per_second': 1.556, 'train_steps_per_second': 0.519, 'train_loss': 0.18771030108133951, 'epoch': 26.67}\n",
            "100% 30/30 [00:57<00:00,  1.93s/it]\n",
            "***** train metrics *****\n",
            "  epoch                    =      26.67\n",
            "  train_loss               =     0.1877\n",
            "  train_runtime            = 0:00:57.84\n",
            "  train_samples            =          3\n",
            "  train_samples_per_second =      1.556\n",
            "  train_steps_per_second   =      0.519\n",
            "训练之后的测试!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
            "打印测试集\n",
            "{'____comment____': '这个键值是注释,其中prompt是当前问题,response是当前回答.history是一个数组内容是[问题1,答案1,问题2,答案2,....]', 'prompt': '长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线', 'response': '用电脑能读数据流吗？水温多少', 'history': []}\n",
            "{'____comment____': None, 'prompt': '95', 'response': '上下水管温差怎么样啊？空气是不是都排干净了呢？', 'history': [['长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线', '用电脑能读数据流吗？水温多少']]}\n",
            "{'____comment____': None, 'prompt': '是的。上下水管都好的', 'response': '那就要检查线路了，一般风扇继电器是由电脑控制吸合的，如果电路存在断路，或者电脑坏了的话会出现继电器不吸合的情况！', 'history': [['长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线', '用电脑能读数据流吗？水温多少'], ['95', '上下水管温差怎么样啊？空气是不是都排干净了呢？']]}\n",
            "Dataset({\n",
            "    features: ['____comment____', 'prompt', 'response', 'history'],\n",
            "    num_rows: 3\n",
            "}) 333333333333333333333333333333333333\n",
            "下面打印评测结果在测试集上全部进行测试\n",
            "第 0 条数据进行测是\n",
            "问题是 长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线 历史是 []\n",
            "答案是 用电脑能读数据流吗？水温多少\n",
            "[INFO|configuration_utils.py:575] 2023-05-22 03:10:10,130 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.27.1\"\n",
            "}\n",
            "\n",
            "回答是 用电脑能读数据流吗？水温多少\n",
            "第 1 条数据进行测是\n",
            "问题是 95 历史是 [['长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线', '用电脑能读数据流吗？水温多少']]\n",
            "答案是 上下水管温差怎么样啊？空气是不是都排干净了呢？\n",
            "[INFO|configuration_utils.py:575] 2023-05-22 03:10:12,168 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.27.1\"\n",
            "}\n",
            "\n",
            "回答是 上下水管温差怎么样啊？空气是不是都排干净了呢？\n",
            "第 2 条数据进行测是\n",
            "问题是 是的。上下水管都好的 历史是 [['长城h3风扇不转。继电器好的。保险丝好的传感器新的风扇也新的这是为什么。就是继电器缺一个信号线', '用电脑能读数据流吗？水温多少'], ['95', '上下水管温差怎么样啊？空气是不是都排干净了呢？']]\n",
            "答案是 那就要检查线路了，一般风扇继电器是由电脑控制吸合的，如果电路存在断路，或者电脑坏了的话会出现继电器不吸合的情况！\n",
            "[INFO|configuration_utils.py:575] 2023-05-22 03:10:14,895 >> Generate config GenerationConfig {\n",
            "  \"_from_model_config\": true,\n",
            "  \"bos_token_id\": 130004,\n",
            "  \"eos_token_id\": 130005,\n",
            "  \"pad_token_id\": 3,\n",
            "  \"transformers_version\": \"4.27.1\"\n",
            "}\n",
            "\n",
            "回答是 那就恭喜恭喜，大概空气排干净了，风扇继电器没有接通信号导致不启动呢。\n",
            "测试完毕\n"
          ]
        }
      ]
    }
  ]
}